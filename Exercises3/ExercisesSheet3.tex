\documentclass[12pt,letterpaper]{article}
\usepackage{./preamble}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Edit These for yourself
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\course{Computational Statistics}
\newcommand\hwnumber{2}
\newcommand\userID{Davi Sales Barreira}
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}%

\begin{document}
% \textbf{\Large Worksheet completed with Octave.}

\section*{Exercise 1 (Gibbs Sampler)}
\begin{enumerate}[leftmargin=!,labelindent=5pt]
\item First, let $X' = X^{(t)}$, $X = X^{(t-1)}$, $Y' = Y^{(t)}$ and
$Y = Y^{(t-1)}$
. Then:
$$
K^S((x,y),(x',y')) = \pi_{Y\mid X}(y' \mid x) \pi_{X \mid Y}(x' \mid y')
$$

Then, to show that it is not reversible:
$$
\pi(x,y)K((x,y),(x',y')) = \pi(x,y) \pi(y' \mid x) \pi(x' \mid y')
$$
$$ 
\pi(x',y')K((x',y'),(x,y)) = \pi(x',y') \pi(y \mid x') \pi(x \mid y)
$$
$$
\therefore
$$
$$
\frac{\pi(x,y)K((x,y),(x',y'))}
{\pi(x',y')K((x',y'),(x,y))} =
\frac{\pi(x,y) \pi(y' \mid x) \pi(x' \mid y')}
{\pi(x',y') \pi(y \mid x') \pi(x \mid y)} = 
\frac{\pi(y)\pi(y' \mid x)}
{\pi(y')\pi(y \mid x')} \neq 1
$$
Therefore, it is not reversible.
\qed

\item First, the kernel expression:
$$
K(x, x') = \int \pi(y' \mid x) \pi(x' \mid y') dy'
$$

Now, let's show that it is $\pi_X$-reversible.
$$
\pi(x')K(x', x) = \pi(x') \int \pi(y \mid x') \pi(x \mid y) dy
=
\int \pi(x') \frac{\pi(y,x')}{\pi(x')}\pi(x\mid y) dy =
$$
$$
=
\int \pi(y,x') \pi(x \mid y) dy = 
\int \pi(y,x') \frac{\pi(x,y)}{\pi(y)}dy =
\int \pi(x,y) \frac{\pi(x',y)}{\pi(y)}dy =
$$
$$
= \pi(x)\int \pi(y \mid x) \pi(x' \mid y)  dy = \pi(x)K(x,x')
$$
\qed

\item First, the kernel expression is:
$$
K^R((x,y),(x',y')) = 
\pi(y' \mid x)\pi(x' \mid y')0.5 + \pi(x' \mid y)\pi(y' \mid x')0.5
$$
Note that it is half the density of sampling first from $y$ plus 
half the density of sampling first from $x$.

Now, let's show that it is reversible:
$$
\frac{\pi(x,y)[\pi(y' \mid x)\pi(x' \mid y')0.5 +
\pi(x' \mid y)\pi(y' \mid x')0.5]}
{
\pi(x',y')[\pi(y \mid x')\pi(x \mid y')0.5 +
\pi(x \mid y')\pi(y \mid x)0.5 ]
} = 
$$
$$ =
\frac{
	\frac{\pi(y' \mid x)}
	{\pi(y')}
	+
	\frac{\pi(x' \mid y)}
	{\pi(x')}
}
{
	\frac{\pi(y \mid x')}
	{\pi(y)}
	+
	\frac{\pi(x \mid y')}
	{\pi(x)}
} = 
1
$$
\qed
\end{enumerate}

\newpage
\section*{Exercise 2 (Metropolis-within-Gibbs)}
\begin{enumerate}[leftmargin=!,labelindent=5pt]
\item Note that:
$$
\alpha(X_1 \mid X_1^{(t-1)}, X_2^{(t-2)}) =
min \left\{
1,
\frac{\pi(X_1' , X_2^{(t-1)})\pi(X_1^{(t-1)}\mid X_2^{(t-1)})}
{\pi(X_1^{(t-1)} , X_2^{(t-1)})\pi(X_1'\mid X_2^{(t-1)})}
\right\} = min \{1, 1\}
$$
Therefore, we get a systematic scan Gibbs sampler, where one
samples $X^t_1 \sim \pi(\cdot \mid X_2^{(t-1)})$, then we accept,
since $\alpha=1$, and finally sample $X_2^t \sim
\pi(\cdot \mid X_1^{(t)})$.
\qed

\item First, let's write the kernel. Since we only accept or reject
the variabel $X_1$, the kernel is the M-H kernel multiplied by the
probability density function of $\pi_{X_2\mid X_1}(X_2\mid X_1)$. Let
$X_1^t, X_2^t = Y_1, Y_2$:
$$
K((x_1,x_2),(y_1, y_2)) =
(q(y_1 \mid x_1, x_2)\alpha(y_1 \mid x_1,x_2)) +
(1 - a(y_1 \mid x_1, x_2))\delta_{y_1}(x_1)\pi_{(Y_2\mid Y_1}(y_2 \mid y_1)
$$
Note that $\alpha = 1$. With that, we show that the kernel is invarant:

$$
\int \int
K((x_1,x_2),(y_1, y_2))\pi(x_1,x_2) dx_1 dx_2 =
\int \int \pi(y_1 \mid x_2)\pi(y_2 \mid y_1) \pi(x_1, x_2)dx_1 dx_2 =
$$
$$=
\int \pi(y_1 \mid x_2) \pi(y_2 \mid y_1) \pi(x_2) dx_2 =
\int \pi(y_1, x_2)\pi(y_2 \mid y_1) dx_2 = \pi(y_1, y_2)
$$
\qed

\end{enumerate}

\newpage
\section*{Exercise 3 (Metropolis-Hastings and Gibbs Sampler)}
\begin{enumerate}[leftmargin=!,labelindent=5pt]
\item  Let's show that the chain is reversible. If $x=y$, it is trivially
reversible. If $x \neq y$, then:
$$
T(x,y) \pi(x) = \alpha(x,y)q(x,y)\pi(x) =
\frac{\gamma(x,y)}{\pi(x)q(x,y)}q(x,y)\pi(x) = \gamma(y,x) =
$$
$$
= \alpha(y,x) q(y,x) \pi(y) = T(y,x) \pi(y)
$$
\qed

\item First, let's verify that it is the M-H algorithm:
$$
\alpha = \frac{\gamma(x,y)}{\pi(x)q(x,y)} = 
\frac{min \{  \pi(x) q(x,y), \pi(y) q(y,x) \}}
{\pi(x) q(x,y)} =
min \left \{
	1, \frac{\pi(y)q(y,x)}
	{\pi(x)q(x,y)}
\right\}
$$

Now, let's give the Barker acceptance ratio:
$$
\alpha(x,y) = \frac{\pi(x)q(x,y)\pi(y)q(y,x)}
{\pi(x)q(x,y)+\pi(y)q(y,x)} =
\frac{\pi(y)q(y,x)}{\pi(x)q(x,y) + \pi(y)q(y,x)}
$$
\qed

\item Let's consider $x \neq y$. Note that:
$$
\frac{1}{\pi(x)q(x,y)}
\geq
\frac{1}{\pi(x)q(x,y) + \pi(y)q(y,x)}
$$
$$
\therefore
$$
$$
\frac{\pi(y)q(y,x)q(x,y)}{\pi(x)q(x,y)}
\geq
\frac{q(x,y)\pi(y)q(y,x)}{\pi(x)q(x,y) + \pi(y)q(y,x)}
$$

Finally, if $\frac{\pi(y)q(y,x)q(x,y)}{\pi(x)q(x,y)} \leq 1$, then:

$$
min\left \{
1,
\frac{\pi(y)q(y,x)q(x,y)}{\pi(x)q(x,y)}
\right\}
\geq
\frac{q(x,y)\pi(y)q(y,x)}{\pi(x)q(x,y) + \pi(y)q(y,x)}
$$

We conclude that the M-H algorithm provides estimators with
lower asymptotic variance than Barker's algorithm.


\end{enumerate}

\end{document}
