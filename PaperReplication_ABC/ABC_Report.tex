% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath, bm}
\usepackage{booktabs} % For pretty tables
\usepackage{caption} % For caption spacing
\usepackage{subcaption} % For sub-figures
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage[all]{nowidow}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{er,positioning,bayesnet}
\usepackage{multicol}
% \usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode,algorithmicx}
% \usepackage{minted}
\usepackage{hyperref}
\usepackage[inline]{enumitem} % Horizontal lists
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\card}[1]{\left\vert{#1}\right\vert}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\definecolor{blue}{HTML}{1F77B4}
\definecolor{orange}{HTML}{FF7F0E}
\definecolor{green}{HTML}{2CA02C}

\pgfplotsset{compat=1.14}

\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\textfraction}{0.1}
\setlength{\floatsep}{3pt plus 1pt minus 1pt}
\setlength{\textfloatsep}{3pt plus 1pt minus 1pt}
\setlength{\intextsep}{3pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{2pt plus 1pt minus 1pt}

\begin{document}
%
\title{Approximate Bayesian Computation Overview}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Davi Sales Barreira}
%
%\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{FGV - Escola de Matem√°tica Aplicada, Rio de Janeiro, Brasil\\ 
\email{davisbarreira@gmail.com}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Approximate Bayesian Computation (ABC)
methods are known as likelihood-free techniques, thus are a useful
approach in problems that the likelihood is intractable, e.g., likelihood
not available in closed form, or likelihood too expensive to calculate.
In this article, we present an overview of the method by replicating
the paper Approximate Bayesian computational
methods by \cite{Marin2012}.

\keywords{Approximate Bayesian Computation \and likelihood-free \and
Monte Carlo.}
\end{abstract}
%
%
%
\section{Introduction}
\subsection{Original ABC} \label{subsec:statistical-summaries}

The Approximate Bayesian Computation method was originally described
by \citet{Rubin1984} as a thought experiment to explain how to sample
from a posterior distribution with a frequency interpretation.
The method became proeminent due to the fact that it circumvents
the need to calculate the likelihood function in order to
obtain the posterior distribution. This can be a very useful
feature in scenarios where the likelihood is intractable or
too expensive to calculate. One example is in the case where
one has latent variables, thus, the likelihood is expressed as:

\begin{equation}
  \ell(\bm\theta \mid \bm y) =
  \bm\int \ell^*(\bm\theta \mid \bm y, \bm u) d\bm u
\end{equation}
with $\bm y$ being the observed variable,
$\bm u$ the latent variable and $\bm\theta$ is the parameter of interest.

\citet{Tavare505} introduced the ABC algorithm as a rejection
techinique to obtain the posterior distribution without the explicit
calculation of the likelihood. This original algorithm is given below.

\begin{algorithm}[H]
\SetAlgoLined
\For{i=1 to N}{
 \Repeat{$\bm y = \bm z$}{
    Sample $\bm\theta' \sim \pi(\cdot)$

    Generate $\bm z \sim p(\cdot \mid \bm\theta')$
 }
  
}
 \caption{Original ABC method}
\end{algorithm}

The proof that the algorithm indeed results in an iid sample
from the posterior is shown below. Let $\bm y$ be the observed,
$\bm \theta$ the parameter of interest and $\bm z$ the generated
samples.

\begin{equation}
  f(\bm \theta_i) \propto \sum_{\bm z \in \mathbb{D}}
  \pi(\bm \theta_i) p(\bm z \mid \bm \theta_i) \mathbb I_{\bm y}(\bm z)
  = \pi(\bm \theta_i) p(\bm y \mid \bm \theta_i) \propto
  \pi(\bm \theta_i \mid \bm y)
\end{equation}

The original ABC formulation only works for
the case where $\bm y$ is discrete
taking finite values, and therefore, an exact match is possible
to be obtained in a finite number of simulations. \citet{Pritchard1999}
then extended the method to a more general form considering an
approximation instead of an exact match. This extended algorithm is
shown below, where

\begin{itemize}
  \item[--] $\eta$ is a function defining a statistic (e.g. the mean),
  \item[--] $\rho$ is a distance function,
  \item[--] $\epsilon$ is an acceptance tolerance.
\end{itemize}

\begin{algorithm}[H]
\SetAlgoLined
\For{i=1 to N}{
 \Repeat{$\rho[\eta(\bm y) , \eta (\bm z)] \leq \epsilon$}{
    Sample $\bm\theta' \sim \pi(\cdot)$

    Generate $\bm z \sim p(\cdot \mid \bm\theta')$
 }
  
}
 \caption{ABC method for discrete and continuous distributions}
\end{algorithm}

For this ABC algorithm, instead of the actual posterior,
we get

\begin{equation}
\pi_\epsilon(\bm \theta, \bm z \mid \bm y) = 
\frac{\pi(\bm \theta) p(\bm z \mid \bm \theta)
\mathbb I_{A_{\epsilon,\bm y}}(\bm z)}
{\int_{A_{\epsilon,\bm y}\times \bm\theta}\pi(\bm \theta)
p(\bm z \mid \bm \theta)d\bm z d \bm \theta}
\end{equation}
where, $A_{\epsilon,\bm y} = \{
\bm z \in \mathcal D \mid \rho[\eta(\bm z), \eta(\bm y) \leq \epsilon]
\}$.
Hence, for a tolerance ($\epsilon$) "small enough", we expect a good
approximation of the real posterior.
\begin{equation}
\pi_\epsilon(\bm \theta \mid \bm y) = 
\int \pi_\epsilon(\bm \theta, \bm z \mid \bm y) d \bm z \approx
\pi(\bm \theta \mid \bm y)
\end{equation}


\subsection{Moving Average} \label{subsec:statistical-summaries}

We will use the Moving Average model, also denoted as MA(q),
for assessing the performance of the ABC methods. The MA(q) process
is a stochastic process defined by:

\begin{equation}
y_k = u_k + \sum_{i=1}^q \theta_i u_{k-i}
\end{equation}
where $(u_k)_{k \in \mathbb Z} \overset{iid}{\sim} N(0,1)$.
The true posterior distribution of MA(2) and MA(1) models can be
numerically computed, since the likelihood function is indeed avaliable.
Therefore, the approximations obtained through ABC can be compared with
the true posterior.

For a $q=2$, imposing the standard identifiability condition
we obtain the following conditions:

\begin{equation}
-2 < \theta_1 < 2, \quad \quad \theta_1+\theta_2 > -1, \quad \quad
\theta_1 - \theta_2 < 1.
\end{equation}

Hence, we use an uniform distribution over this triangular region as
prior for $\bm \theta$. The likelihood of $\bm y \mid \bm \theta$ is
more complex because of the need to integrate $\bm u$.


% \section{Moving Average}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{apa}
\bibliography{abc}
%
\end{document}
