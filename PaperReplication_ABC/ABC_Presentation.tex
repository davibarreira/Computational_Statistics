\documentclass[10pt]{beamer}

\usepackage{natbib}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\usepackage{amsmath, bm}
\usepackage[ruled,vlined]{algorithm2e}

\title{Approximate Bayesian Computation}
\subtitle{}
% \date{\today}
\date{}
\author{Davi Barreira}
\institute{FGV - Escola de Matem√°tica Aplicada}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\AtBeginSection{}
\section[Objective Motivation]{Objective \& Motivation}
\begin{frame}[fragile]{Objective \& Motivation}

  The objective of this presentation is to give an overview of
  the Approximate Bayesian Computation (ABC) algorithm through the
  replication of the
  paper \textbf{Approximate Bayesian computational methods} by
  \cite{Marin2012}.

  The paper talks about different variants of ABC by estimating the
  posterior of Moving Average models.

\end{frame}

\begin{frame}[fragile]{Objective \& Motivation}

  ABC methods are known as likelihood-free techniques, thus are
  a useful approach in problems that the likelihood is intractable, e.g., likelihood not available in
  closed form, or likelihood too expensive to calculate.
  \begin{itemize}
    \item Coalecent models in population genetics \citep{Tavare505};
    \item Species dynamics \citep{Jabot2016};
    \item Real-world model of HIV transmission \citep{McKinley2018}.
  \end{itemize}

\end{frame}

\begin{frame}[fragile]{Objective \& Motivation}

  In some settings where we have latent variables, the likelihood is
  expressed as:
  $$\ell(\bm\theta \mid \bm y) =
  \bm\int \ell^*(\bm\theta \mid \bm y, \bm u) d\bm u$$

  Hence, $\bm y$ is observed and $\bm u$ is latent and $\bm\theta$
  is the parameter of interest.

\end{frame}

\AtBeginSection{}
\section[Original]{Original ABC Algorithm}
\begin{frame}[fragile]{Original ABC Algorithm}

  \citet{Rubin1984} described the ABC algorithm as a thought experiment
  to explain how to sample from a posterior distribution.
  \citet{Tavare505} is usually considered the paper responsible for the
  proposing ABC for infering the posterior distribution.

  \vspace{1cm}

\begin{algorithm}[H]
\SetAlgoLined
\For{i=1 to N}{
 \Repeat{$\bm y = \bm z$}{
    Sample $\bm\theta' \sim \pi(\cdot)$

    Generate $\bm z \sim p(\cdot \mid \bm\theta')$
 }
  
}
 \caption{Original ABC method}
\end{algorithm}


\end{frame}

\begin{frame}[fragile]{Original ABC Algorithm}

  Below we have an schematic drawing with an example of
  the ABC method for Beta/Binomial model.
    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{images/Vis-ABC.png}
        \caption{Schematic drawing of ABC method for Beta/Binomial
        model
        }
    \end{figure}

\end{frame}

\begin{frame}[fragile]{Original ABC Algorithm}

  The proof that the algorithm indeed results in an iid sample
  from the posterior is shown below. Let $\bm y$ be the observed,
  $\bm \theta$ the parameter of interest and $\bm z$ the generated
  samples.
  $$
  p(\bm \theta_i) \propto \sum_{\bm z \in \mathbb{D}}
  \pi(\bm \theta_i) p(\bm z \mid \bm \theta_i) \mathbb I_{\bm y}(\bm z)
  = \pi(\bm \theta_i) p(\bm y \mid \bm \theta_i) \propto
  \pi(\bm \theta_i \mid \bm y)
  $$

\end{frame}

\begin{frame}[fragile]{Original ABC Algorithm}

  \citet{Pritchard1999} extended the original algorithm to the case
  of continuos sample spaces.

  \vspace{0.3cm}

\begin{algorithm}[H]
\SetAlgoLined
\For{i=1 to N}{
 \Repeat{$\rho[\eta(\bm y) , \eta (\bm z)] \leq \epsilon$}{
    Sample $\bm\theta' \sim \pi(\cdot)$

    Generate $\bm z \sim p(\cdot \mid \bm\theta')$
 }
  
}
 \caption{ABC method for discrete and continuous distributions}
\end{algorithm}
\begin{itemize}
  \item[--] $\eta$: function defining a statistic (e.g. the mean),
  \item[--] $\rho$: a distance function,
  \item[--] $\epsilon$: acceptance tolerance.
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Original ABC Algorithm}

  For this ABC algorithm, instead of the actual posterior,
  we get
  $$
  \pi_\epsilon(\bm \theta, \bm z \mid \bm y) = 
  \frac{\pi(\bm \theta) p(\bm z \mid \bm \theta)
  \mathbb I_{A_{\epsilon,\bm y}}(\bm z)}
  {\int_{A_{\epsilon,\bm y}\times \bm\theta}\pi(\bm \theta)
  p(\bm z \mid \bm \theta)d\bm z d \bm \theta}
  $$
  Where, $A_{\epsilon,\bm y} = \{
  \bm z \in \mathbb D \mid \rho[\eta(\bm z), \eta(\bm y) \leq \epsilon].
  \}$

  Hence, for a tolerance ($\epsilon$) "small enough", we expect a good
  approximation.
  $$\pi_\epsilon(\bm \theta \mid \bm y) = 
  \int \pi_\epsilon(\bm \theta, \bm z \mid \bm y) d \bm z \approx
  \pi(\bm \theta \mid \bm y)$$

\end{frame}

\AtBeginSection{}
\section[Moving Average]{Moving Average}
\begin{frame}[fragile]{Moving Average}

  We will use the Moving Average model, also denoted as MA(q),
  for assessing the performance of the ABC methods. The MA(q) process
  is a stochastic process defined by:
  $$y_k = u_k + \sum_{i=1}^q \theta_i u_{k-i}$$
  Where $(u_k)_{k \in \mathbb Z} \overset{iid}{\sim} N(0,1)$.
  For a $q=2$, imposing the standard identifiability condition
  we obtain the following conditions:
  $$
  -2 < \theta_1 < 2, \quad \quad \theta_1+\theta_2 > -1, \quad \quad
  \theta_1 - \theta_2 < 1.
  $$
  Hence, we use an uniform distribution over this triangular region as
  prior for $\bm \theta$. The likelihood of $\bm y \mid \bm \theta$ is
  more complex because of the need to integrate $\bm u$.

\end{frame}

\begin{frame}[fragile]{Moving Average}
  
  We generate a synthetic sample of length 100 using
  $(\theta_1, \theta_2) = (0.6, 0.2)$. For $q=2$ we can also
  numerically calculate the real posterior and the marginal distributions.

  $$
  \pi(\bm\theta \mid \bm y) \propto \pi(\bm\theta)
  p(\bm y \mid \bm \theta), \quad \quad
  \bm y \mid \bm \theta \sim MVN(0, \Sigma) \quad
  $$

  $$
  \Sigma =
  \tiny{
  \begin{bmatrix}
   1+\theta_1^2 + \theta_2^2    & \theta_1 + \theta_2 \theta_1 & \theta_2                     & 0                            & 0        & 0 & ... & 0 \\
   \theta_1 + \theta_2 \theta_1 & 1+\theta_1^2 + \theta_2^2    & \theta_1 + \theta_2 \theta_1 & \theta_2                     & 0        & 0 & ... & 0 \\
   \theta_2                     & \theta_1 + \theta_2 \theta_1 & 1+\theta_1^2 + \theta_2^2    & \theta_1 + \theta_2 \theta_1 & \theta_2 & 0 & ... & 0 \\
   0               & \theta_2   & \theta_1 + \theta_2 \theta_1 & 1+\theta_1^2 + \theta_2^2    & \theta_1 + \theta_2 \theta_1 & \theta_2 &... & 0 \\
   \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
   0 & 0 & 0 & 0 & 0 & \theta_2 & \theta_1 + \theta_1\theta_2 & 1+\theta_1^2 + \theta_2^2 \\
  \end{bmatrix}}
  $$

\end{frame}

% \appendix

\begin{frame}[allowframebreaks]{References}

% \renewcommand{\bibsection}{\section{}}
  \renewcommand{\section}[2]{}%
  \bibliography{abc}
  % \bibliographystyle{plainnat}
  % \bibliographystyle{plain}
  % \bibliographystyle{abbrv}
  \bibliographystyle{apa}

\end{frame}

\end{document}
